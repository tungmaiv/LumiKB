# LumiKB Production Docker Compose
# Story 7.4: Production Deployment Configuration
# All services with production settings: resource limits, restart policies, health checks, log rotation
#
# Usage:
#   cp .env.prod.template .env.prod
#   # Edit .env.prod with production secrets
#   docker compose -f docker-compose.prod.yml --env-file .env.prod up -d
#
# Prerequisites:
#   - All secrets must be set in .env.prod (see .env.prod.template)
#   - Docker Compose v2.0+
#   - Pre-built images: lumikb-api:latest, lumikb-worker:latest, lumikb-beat:latest, lumikb-frontend:latest

services:
  # =============================================================================
  # PostgreSQL - Primary Database
  # =============================================================================
  postgres:
    image: postgres:16
    container_name: lumikb-postgres-prod
    command:
      - "postgres"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "shared_buffers=256MB"
      - "-c"
      - "effective_cache_size=768MB"
      - "-c"
      - "maintenance_work_mem=128MB"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "work_mem=4MB"
      - "-c"
      - "min_wal_size=1GB"
      - "-c"
      - "max_wal_size=4GB"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - lumikb-network
    restart: always
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 1G
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"

  # =============================================================================
  # Redis - Cache, Sessions, and Message Broker
  # =============================================================================
  redis:
    image: redis:7-alpine
    container_name: lumikb-redis-prod
    command: >
      redis-server
      --appendonly yes
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - lumikb-network
    restart: always
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"

  # =============================================================================
  # MinIO - S3-Compatible Object Storage
  # =============================================================================
  minio:
    image: minio/minio:latest
    container_name: lumikb-minio-prod
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - lumikb-network
    restart: always
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 1G
        reservations:
          cpus: "0.25"
          memory: 512M
    logging:
      driver: json-file
      options:
        max-size: "30m"
        max-file: "3"

  # =============================================================================
  # Qdrant - Vector Database
  # =============================================================================
  qdrant:
    image: qdrant/qdrant:latest
    container_name: lumikb-qdrant-prod
    volumes:
      - qdrant_data:/qdrant/storage
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'exec 3<>/dev/tcp/localhost/6333 && echo -e \"GET /readyz HTTP/1.0\\r\\n\\r\\n\" >&3 && cat <&3 | grep -q ready'"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - lumikb-network
    restart: always
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 4G
        reservations:
          cpus: "1.0"
          memory: 2G
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"

  # =============================================================================
  # Ollama - Local LLM Runtime with GPU Support
  # =============================================================================
  # Requires: NVIDIA GPU with nvidia-container-toolkit installed
  ollama:
    image: ollama/ollama:latest
    container_name: lumikb-ollama-prod
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
        limits:
          cpus: "4.0"
          memory: 8G
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - lumikb-network
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"

  # =============================================================================
  # LiteLLM - LLM Gateway Proxy
  # =============================================================================
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: lumikb-litellm-prod
    volumes:
      - ./litellm_config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/litellm
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      GEMINI_API_KEY: ${GEMINI_API_KEY:-}
      # Ollama - use Docker service name for container-to-container communication
      OLLAMA_API_BASE: http://ollama:11434
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:4000/health/readiness')\""]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 45s
    networks:
      - lumikb-network
    restart: always
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    logging:
      driver: json-file
      options:
        max-size: "30m"
        max-file: "5"

  # =============================================================================
  # FastAPI Backend - API Server
  # =============================================================================
  api:
    image: ${API_IMAGE:-lumikb-api:latest}
    container_name: lumikb-api-prod
    environment:
      LUMIKB_DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      LUMIKB_REDIS_URL: redis://redis:6379/0
      LUMIKB_CELERY_BROKER_URL: redis://redis:6379/0
      LUMIKB_CELERY_RESULT_BACKEND: redis://redis:6379/0
      LUMIKB_MINIO_ENDPOINT: minio:9000
      LUMIKB_MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      LUMIKB_MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      LUMIKB_MINIO_SECURE: "false"
      LUMIKB_QDRANT_HOST: qdrant
      LUMIKB_QDRANT_PORT: "6333"
      LUMIKB_LITELLM_URL: http://litellm:4000
      LUMIKB_LITELLM_API_KEY: ${LITELLM_MASTER_KEY}
      LUMIKB_SECRET_KEY: ${SECRET_KEY}
      LUMIKB_JWT_SECRET: ${JWT_SECRET}
      LUMIKB_ENVIRONMENT: production
      LUMIKB_DEBUG: "false"
    ports:
      - "${API_PORT:-8000}:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - lumikb-network
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      litellm:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
        reservations:
          cpus: "0.5"
          memory: 512M
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"

  # =============================================================================
  # Celery Worker - Document Processing
  # =============================================================================
  celery-worker:
    image: ${WORKER_IMAGE:-lumikb-worker:latest}
    container_name: lumikb-celery-worker-prod
    command: >
      celery -A app.workers.celery_app worker
      --loglevel=info
      --queues=default,document_processing
      --concurrency=4
    environment:
      LUMIKB_DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      LUMIKB_REDIS_URL: redis://redis:6379/0
      LUMIKB_CELERY_BROKER_URL: redis://redis:6379/0
      LUMIKB_CELERY_RESULT_BACKEND: redis://redis:6379/0
      LUMIKB_MINIO_ENDPOINT: minio:9000
      LUMIKB_MINIO_ACCESS_KEY: ${MINIO_ROOT_USER}
      LUMIKB_MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD}
      LUMIKB_MINIO_SECURE: "false"
      LUMIKB_QDRANT_HOST: qdrant
      LUMIKB_QDRANT_PORT: "6333"
      LUMIKB_LITELLM_URL: http://litellm:4000
      LUMIKB_LITELLM_API_KEY: ${LITELLM_MASTER_KEY}
      LUMIKB_ENVIRONMENT: production
    healthcheck:
      test: ["CMD-SHELL", "celery -A app.workers.celery_app inspect ping -d celery@$HOSTNAME --timeout 10"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    networks:
      - lumikb-network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      litellm:
        condition: service_healthy
    restart: always
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "1.0"
          memory: 1G
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "10"

  # =============================================================================
  # Celery Beat - Periodic Task Scheduler
  # =============================================================================
  celery-beat:
    image: ${BEAT_IMAGE:-lumikb-beat:latest}
    container_name: lumikb-celery-beat-prod
    command: >
      celery -A app.workers.celery_app beat
      --loglevel=info
      --schedule=/app/celery-data/celerybeat-schedule
    environment:
      LUMIKB_DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      LUMIKB_REDIS_URL: redis://redis:6379/0
      LUMIKB_CELERY_BROKER_URL: redis://redis:6379/0
      LUMIKB_CELERY_RESULT_BACKEND: redis://redis:6379/0
      LUMIKB_ENVIRONMENT: production
    volumes:
      - celery_beat_data:/app/celery-data
    healthcheck:
      test: ["CMD-SHELL", "test -f /app/celery-data/celerybeat-schedule || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - lumikb-network
    depends_on:
      redis:
        condition: service_healthy
    restart: always
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"

  # =============================================================================
  # Prometheus - Metrics Collection (Story 7.5: AC-7.5.1)
  # =============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: lumikb-prometheus-prod
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
    volumes:
      - ../prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - lumikb-network
    restart: always
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 1G
        reservations:
          cpus: "0.25"
          memory: 512M
    logging:
      driver: json-file
      options:
        max-size: "30m"
        max-file: "3"

  # =============================================================================
  # Grafana - Visualization Dashboards (Story 7.5: AC-7.5.2)
  # =============================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: lumikb-grafana-prod
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:3001}
    volumes:
      - ../grafana/provisioning/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
      - ../grafana/provisioning/dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro
      - ../grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - grafana_data:/var/lib/grafana
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - lumikb-network
    restart: always
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    logging:
      driver: json-file
      options:
        max-size: "30m"
        max-file: "3"

  # =============================================================================
  # AlertManager - Alert Routing (Story 7.5: AC-7.5.3)
  # =============================================================================
  alertmanager:
    image: prom/alertmanager:latest
    container_name: lumikb-alertmanager-prod
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - ../alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    ports:
      - "${ALERTMANAGER_PORT:-9093}:9093"
    environment:
      SMTP_USERNAME: ${SMTP_USERNAME:-}
      SMTP_PASSWORD: ${SMTP_PASSWORD:-}
      PAGERDUTY_WEBHOOK_URL: ${PAGERDUTY_WEBHOOK_URL:-http://localhost}
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL:-http://localhost}
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9093/-/healthy"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 15s
    networks:
      - lumikb-network
    restart: always
    depends_on:
      - prometheus
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
        reservations:
          cpus: "0.1"
          memory: 128M
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"

  # =============================================================================
  # Frontend - Next.js Application
  # =============================================================================
  frontend:
    image: ${FRONTEND_IMAGE:-lumikb-frontend:latest}
    container_name: lumikb-frontend-prod
    environment:
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://api:8000}
      NODE_ENV: production
    ports:
      - "${FRONTEND_PORT:-3000}:3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - lumikb-network
    restart: always
    depends_on:
      api:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M
    logging:
      driver: json-file
      options:
        max-size: "30m"
        max-file: "5"

# =============================================================================
# Networks
# =============================================================================
networks:
  lumikb-network:
    driver: bridge
    name: lumikb-network-prod

# =============================================================================
# Named Volumes for Data Persistence
# =============================================================================
volumes:
  postgres_data:
    name: lumikb-postgres-data-prod
  redis_data:
    name: lumikb-redis-data-prod
  minio_data:
    name: lumikb-minio-data-prod
  qdrant_data:
    name: lumikb-qdrant-data-prod
  celery_beat_data:
    name: lumikb-celery-beat-data-prod
  ollama_data:
    name: lumikb-ollama-data-prod
  prometheus_data:
    name: lumikb-prometheus-data-prod
  grafana_data:
    name: lumikb-grafana-data-prod
  alertmanager_data:
    name: lumikb-alertmanager-data-prod
