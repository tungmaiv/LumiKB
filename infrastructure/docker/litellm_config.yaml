# =============================================================================
# LiteLLM Proxy Configuration for LumiKB
# =============================================================================
# This configuration provides a unified LLM API gateway with model routing,
# fallback handling, and provider abstraction.
#
# Documentation: https://docs.litellm.ai/docs/proxy/configs
# =============================================================================

# =============================================================================
# Model Configuration
# =============================================================================
# Define available models and their routing
# The proxy will use environment variables for API keys

model_list:
  # OpenAI Models
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-4o - Latest multimodal model"
      max_tokens: 128000

  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-4o Mini - Fast and cost-effective"
      max_tokens: 128000

  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI GPT-3.5 Turbo - Fast and affordable"
      max_tokens: 16385

  # Anthropic Models
  - model_name: claude-3-5-sonnet
    litellm_params:
      model: anthropic/claude-3-5-sonnet-latest
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      description: "Anthropic Claude 3.5 Sonnet - Balanced performance"
      max_tokens: 200000

  - model_name: claude-3-5-haiku
    litellm_params:
      model: anthropic/claude-3-5-haiku-latest
      api_key: os.environ/ANTHROPIC_API_KEY
    model_info:
      description: "Anthropic Claude 3.5 Haiku - Fast and efficient"
      max_tokens: 200000

  # =============================================================================
  # Google Gemini Models
  # =============================================================================
  # Generation Models
  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash-exp
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      description: "Google Gemini 2.0 Flash - Latest experimental model"
      max_tokens: 8192

  - model_name: gemini-1.5-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      description: "Google Gemini 1.5 Flash - Fast and efficient"
      max_tokens: 8192

  - model_name: gemini-1.5-pro
    litellm_params:
      model: gemini/gemini-1.5-pro
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      description: "Google Gemini 1.5 Pro - High capability"
      max_tokens: 8192

  # Gemini Embedding Model
  - model_name: gemini-embedding
    litellm_params:
      model: gemini/text-embedding-004
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      description: "Google Gemini Text Embedding - 768 dimensions"

  # =============================================================================
  # Ollama Models (Local)
  # =============================================================================
  # Generation Models
  - model_name: ollama-llama3.2
    litellm_params:
      model: ollama/llama3.2
      api_base: os.environ/OLLAMA_API_BASE
    model_info:
      description: "Ollama Llama 3.2 - Local model"
      max_tokens: 4096

  - model_name: ollama-mistral
    litellm_params:
      model: ollama/mistral
      api_base: os.environ/OLLAMA_API_BASE
    model_info:
      description: "Ollama Mistral - Local model"
      max_tokens: 4096

  - model_name: ollama-gemma3
    litellm_params:
      model: ollama/gemma3:4b
      api_base: os.environ/OLLAMA_API_BASE
    model_info:
      description: "Ollama Gemma3 4B - Local generation model"
      max_tokens: 8192

  # Ollama Embedding Models
  - model_name: ollama-embedding
    litellm_params:
      model: ollama/nomic-embed-text
      api_base: os.environ/OLLAMA_API_BASE
    model_info:
      description: "Ollama Nomic Embed Text - Local embedding model (768 dimensions)"

  - model_name: ollama-mxbai-embed
    litellm_params:
      model: ollama/mxbai-embed-large
      api_base: os.environ/OLLAMA_API_BASE
    model_info:
      description: "Ollama MXBAI Embed Large - Local embedding model (1024 dimensions)"

  # =============================================================================
  # Default Model Aliases for Application Use
  # =============================================================================
  # These are the models LumiKB uses by default
  # Change these to switch between providers

  # Default Generation Model (for chat, synthesis, document generation)
  # Using local Ollama gemma3:4b for offline/free usage
  - model_name: default
    litellm_params:
      model: ollama/gemma3:4b
      api_base: os.environ/OLLAMA_API_BASE
    model_info:
      description: "Default generation model for LumiKB (Ollama local)"

  # Default Embedding Model (for document indexing, search)
  # Using local Ollama nomic-embed-text (768 dimensions, same as Gemini)
  - model_name: embedding
    litellm_params:
      model: ollama/nomic-embed-text
      api_base: os.environ/OLLAMA_API_BASE
    model_info:
      description: "Default embedding model for document processing (Ollama local)"

  # =============================================================================
  # OpenAI Embedding Models
  # =============================================================================
  - model_name: openai-embedding-small
    litellm_params:
      model: openai/text-embedding-3-small
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI Text Embedding 3 Small - 1536 dimensions"

  - model_name: openai-embedding-large
    litellm_params:
      model: openai/text-embedding-3-large
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      description: "OpenAI Text Embedding 3 Large - 3072 dimensions"

# =============================================================================
# Router Settings
# =============================================================================
# Configure load balancing and fallback behavior

router_settings:
  routing_strategy: simple-shuffle
  # CRITICAL: num_retries must be 0 to prevent duplicate streaming requests
  # LiteLLM's retry mechanism sends the request again even on streaming success,
  # causing tokens to be duplicated (e.g., "Here'sHere's a a structured structured")
  num_retries: 0
  timeout: 120
  retry_after: 5

  # NOTE: Fallbacks disabled to prevent duplicate streaming issue
  # The fallback mechanism was causing LiteLLM to make two requests
  # when streaming responses, resulting in duplicated text.
  # fallbacks:
  #   - default: [gemini-1.5-flash, gpt-4o-mini, ollama-llama3.2]
  #   - gpt-4o: [claude-3-5-sonnet, gemini-1.5-pro, gpt-4o-mini]
  #   - gemini-1.5-flash: [gpt-4o-mini, claude-3-5-haiku]
  #   - claude-3-5-sonnet: [gpt-4o, gemini-1.5-pro]

# =============================================================================
# General Settings
# =============================================================================

general_settings:
  # Master key for admin access (set via LITELLM_MASTER_KEY env var)
  master_key: os.environ/LITELLM_MASTER_KEY

  # Enable request/response logging
  store_model_in_db: false

  # Health check configuration
  health_check_interval: 30

# =============================================================================
# Litellm Settings
# =============================================================================

litellm_settings:
  # Drop unsupported params instead of erroring
  drop_params: true

  # Set default max tokens if not specified
  max_tokens: 4096

  # Request timeout
  request_timeout: 120

  # NOTE: Do NOT set stream: true here
  # The backend explicitly controls streaming via the stream= parameter
  # Setting it here causes double-streaming issues
