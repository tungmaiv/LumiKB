<story-context id="7-6-backend-unit-test-fixes" v="1.0">
  <metadata>
    <epicId>7</epicId>
    <storyId>7-6</storyId>
    <title>Backend Unit Test Fixes</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-12-08</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/7-6-backend-unit-test-fixes.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>Developer</asA>
    <iWant>All backend unit tests passing with proper dependency injection mocking</iWant>
    <soThat>The CI pipeline is green and tests reliably validate business logic without external dependencies</soThat>
    <tasks>
      <task id="1" ac="7.6.1">
        <name>Fix draft_service Unit Tests</name>
        <subtasks>
          <subtask id="1.1">Audit test_draft_service.py for database session issues</subtask>
          <subtask id="1.2">Implement proper AsyncSession mock with query result simulation</subtask>
          <subtask id="1.3">Add factory fixtures for Draft model test data</subtask>
          <subtask id="1.4">Verify tests pass in isolation (pytest -k test_draft_service)</subtask>
        </subtasks>
      </task>
      <task id="2" ac="7.6.2">
        <name>Fix search_service Unit Tests</name>
        <subtasks>
          <subtask id="2.1">Audit test_search_service.py for Qdrant client issues</subtask>
          <subtask id="2.2">Create QdrantClient mock with search result fixtures</subtask>
          <subtask id="2.3">Mock embedding generation to avoid external calls</subtask>
          <subtask id="2.4">Verify tests pass in isolation (pytest -k test_search_service)</subtask>
        </subtasks>
      </task>
      <task id="3" ac="7.6.3">
        <name>Fix generation_service Unit Tests</name>
        <subtasks>
          <subtask id="3.1">Audit test_generation_service.py for LiteLLM client issues</subtask>
          <subtask id="3.2">Create LiteLLM completion mock with streaming response simulation</subtask>
          <subtask id="3.3">Add test fixtures for various generation scenarios</subtask>
          <subtask id="3.4">Verify tests pass in isolation (pytest -k test_generation_service)</subtask>
        </subtasks>
      </task>
      <task id="4" ac="7.6.4">
        <name>Fix explanation_service Unit Tests</name>
        <subtasks>
          <subtask id="4.1">Audit test_explanation_service.py for LLM response issues</subtask>
          <subtask id="4.2">Mock LLM responses with realistic explanation content</subtask>
          <subtask id="4.3">Add edge case tests (empty results, timeout simulation)</subtask>
          <subtask id="4.4">Verify tests pass in isolation (pytest -k test_explanation_service)</subtask>
        </subtasks>
      </task>
      <task id="5" ac="7.6.1,7.6.2,7.6.3,7.6.4">
        <name>Fix Additional Failing Unit Tests</name>
        <subtasks>
          <subtask id="5.1">Fix test_audit_service_queries.py - database session mock</subtask>
          <subtask id="5.2">Fix test_queue_monitor_service.py - Redis client mock</subtask>
          <subtask id="5.3">Fix test_kb_recommendation_service.py - database query mock</subtask>
          <subtask id="5.4">Run full unit test suite, fix any remaining failures</subtask>
        </subtasks>
      </task>
      <task id="6" ac="7.6.5">
        <name>Document DI Mock Pattern</name>
        <subtasks>
          <subtask id="6.1">Update docs/testing-guideline.md with DI mock section</subtask>
          <subtask id="6.2">Document AsyncSession mock pattern with examples</subtask>
          <subtask id="6.3">Document external client mock patterns (Qdrant, Redis, LiteLLM)</subtask>
          <subtask id="6.4">Add troubleshooting guide for common mock issues</subtask>
        </subtasks>
      </task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="AC-7.6.1" priority="P0">
      <description>All draft_service unit tests pass with proper database session mocking</description>
      <testable>true</testable>
      <verificationMethod>Run: pytest backend/tests/unit/test_draft_service.py -v - all tests pass</verificationMethod>
    </criterion>
    <criterion id="AC-7.6.2" priority="P0">
      <description>All search_service unit tests pass with Qdrant client mocking</description>
      <testable>true</testable>
      <verificationMethod>Run: pytest backend/tests/unit/test_search_service.py -v - all tests pass</verificationMethod>
    </criterion>
    <criterion id="AC-7.6.3" priority="P0">
      <description>All generation_service unit tests pass with LiteLLM client mocking</description>
      <testable>true</testable>
      <verificationMethod>Run: pytest backend/tests/unit/test_generation_service.py -v - all tests pass</verificationMethod>
    </criterion>
    <criterion id="AC-7.6.4" priority="P0">
      <description>All explanation_service unit tests pass with LLM response mocking</description>
      <testable>true</testable>
      <verificationMethod>Run: pytest backend/tests/unit/test_explanation_service.py -v - all tests pass</verificationMethod>
    </criterion>
    <criterion id="AC-7.6.5" priority="P1">
      <description>DI mock pattern documented in testing guidelines</description>
      <testable>true</testable>
      <verificationMethod>docs/testing-guideline.md contains "DI Mock Pattern" section with AsyncSession, Qdrant, Redis, LiteLLM examples</verificationMethod>
    </criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc path="docs/testing-guideline.md">
        <relevance>Testing guidelines to be updated with DI mock patterns</relevance>
        <keyInfo>
          - pytest-asyncio mode: auto
          - Test markers: unit, integration, e2e
          - Coverage target: maintain >80%
        </keyInfo>
      </doc>
      <doc path="docs/sprint-artifacts/tech-spec-epic-7.md">
        <relevance>Story 7-6 acceptance criteria and tech debt reference</relevance>
        <keyInfo>
          - TD-5.15-1: 3 failing backend unit tests from Epic 5
          - DI mock pattern documentation requirement
        </keyInfo>
      </doc>
    </docs>
    <code>
      <existing>
        <file path="backend/tests/unit/test_draft_service.py">
          <purpose>Draft service unit tests</purpose>
          <status>EXISTS - 12 tests, uses AsyncMock for repository</status>
          <keyPatterns>
            <pattern>DraftService(draft_repository=mock_repo) - constructor injection</pattern>
            <pattern>mock_repo = AsyncMock() for async repository methods</pattern>
            <pattern>create_draft() factory from tests/factories</pattern>
            <pattern>Tests: creation, status transitions, citation validation, retrieval, deletion</pattern>
          </keyPatterns>
        </file>
        <file path="backend/tests/unit/test_search_service.py">
          <purpose>Search service unit tests</purpose>
          <status>EXISTS - ~25 tests covering embedding, search, synthesis</status>
          <keyPatterns>
            <pattern>patch("app.services.search_service.qdrant_service") for Qdrant mock</pattern>
            <pattern>patch("app.services.search_service.embedding_client") for LiteLLM mock</pattern>
            <pattern>patch("app.services.search_service.RedisClient.get_client") for cache</pattern>
            <pattern>MagicMock for sync methods, AsyncMock for async</pattern>
            <pattern>Tests: embed_query, search_collections, cache hit/miss, permissions, audit</pattern>
          </keyPatterns>
        </file>
        <file path="backend/tests/unit/test_generation_service.py">
          <purpose>Generation service unit tests</purpose>
          <status>EXISTS - 7 tests for streaming generation</status>
          <keyPatterns>
            <pattern>GenerationService() without constructor injection</pattern>
            <pattern>patch.object(service.llm_client, "chat_completion") for LLM mock</pattern>
            <pattern>mock_session fixture for AsyncSession</pattern>
            <pattern>_create_llm_chunk helper for streaming response simulation</pattern>
            <pattern>Tests: insufficient sources, status events, token events, citations, done event</pattern>
          </keyPatterns>
        </file>
        <file path="backend/tests/unit/test_explanation_service.py">
          <purpose>Explanation service unit tests</purpose>
          <status>EXISTS - 8 tests for relevance explanations</status>
          <keyPatterns>
            <pattern>ExplanationService(qdrant_service=mock, redis_client=mock) - constructor DI</pattern>
            <pattern>patch("app.services.explanation_service.acompletion") for LiteLLM</pattern>
            <pattern>mock_qdrant.client.retrieve = AsyncMock for vector retrieval</pattern>
            <pattern>Tests: keyword extraction, fallback on timeout, related docs, caching</pattern>
          </keyPatterns>
        </file>
        <file path="backend/tests/factories/draft_factory.py">
          <purpose>Factory functions for test data</purpose>
          <status>EXISTS - provides create_draft, create_citation helpers</status>
        </file>
      </existing>
      <toFix>
        <file path="backend/tests/unit/test_audit_service_queries.py">
          <issue>Database session mock needs AsyncSession spec</issue>
          <fix>Use AsyncMock(spec=AsyncSession) with proper execute chain</fix>
        </file>
        <file path="backend/tests/unit/test_queue_monitor_service.py">
          <issue>Redis client mock needs proper async context</issue>
          <fix>Mock RedisClient.get_client with AsyncMock returning async client</fix>
        </file>
        <file path="backend/tests/unit/test_kb_recommendation_service.py">
          <issue>Database query mock not returning proper result objects</issue>
          <fix>Mock execute().scalars().all() chain with MagicMock</fix>
        </file>
      </toFix>
    </code>
    <dependencies>
      <backend>
        <package name="pytest" version=">=8.0.0" status="EXISTS">
          <purpose>Test framework</purpose>
        </package>
        <package name="pytest-asyncio" version=">=0.24.0" status="EXISTS">
          <purpose>Async test support with auto mode</purpose>
        </package>
        <package name="pytest-cov" version=">=5.0.0" status="EXISTS">
          <purpose>Coverage reporting</purpose>
        </package>
      </backend>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="testing">
      Unit tests must NOT make network/database calls - all external dependencies mocked
    </constraint>
    <constraint type="testing">
      Each test must be isolated - no shared mutable state between tests
    </constraint>
    <constraint type="testing">
      Coverage target: maintain >80% coverage on fixed tests
    </constraint>
    <constraint type="pattern">
      Mock at boundary: Mock external clients (Qdrant, Redis, LiteLLM), not internal logic
    </constraint>
    <constraint type="pattern">
      Use AsyncMock for async methods, MagicMock for sync methods
    </constraint>
    <constraint type="pattern">
      Factory Pattern: Use pytest factories for consistent test data generation
    </constraint>
  </constraints>

  <interfaces>
    <mockPattern name="AsyncSession">
      <description>SQLAlchemy async database session mock</description>
      <code><![CDATA[
@pytest.fixture
def mock_db_session():
    session = AsyncMock(spec=AsyncSession)
    result = MagicMock()
    result.scalars.return_value.all.return_value = []
    result.scalars.return_value.first.return_value = None
    session.execute = AsyncMock(return_value=result)
    session.commit = AsyncMock()
    session.refresh = AsyncMock()
    return session
      ]]></code>
    </mockPattern>
    <mockPattern name="QdrantClient">
      <description>Qdrant vector database client mock</description>
      <code><![CDATA[
@pytest.fixture
def mock_qdrant_client():
    client = MagicMock(spec=QdrantClient)
    # For search operations
    mock_result = MagicMock()
    mock_result.score = 0.95
    mock_result.payload = {"document_id": "doc-1", "chunk_text": "test"}
    client.search.return_value = [mock_result]
    # For retrieve operations (with vectors)
    mock_point = MagicMock()
    mock_point.vector = [0.1] * 1536
    mock_point.payload = {"chunk_id": "chunk-1"}
    client.retrieve.return_value = [mock_point]
    return client
      ]]></code>
    </mockPattern>
    <mockPattern name="LiteLLM">
      <description>LiteLLM completion mock for embedding and generation</description>
      <code><![CDATA[
# For embeddings
with patch("app.services.search_service.embedding_client") as mock_client:
    mock_client.get_embeddings = AsyncMock(return_value=[[0.1] * 1536])

# For chat completion
mock_response = MagicMock()
mock_response.choices = [MagicMock()]
mock_response.choices[0].message.content = "Test response"
mock_client.chat_completion = AsyncMock(return_value=mock_response)

# For streaming
def _create_llm_chunk(content: str):
    chunk = MagicMock()
    chunk.choices = [MagicMock()]
    chunk.choices[0].delta.content = content
    return chunk
      ]]></code>
    </mockPattern>
    <mockPattern name="Redis">
      <description>Redis client mock for caching operations</description>
      <code><![CDATA[
with patch("app.services.search_service.RedisClient.get_client") as mock_redis:
    mock_redis_instance = AsyncMock()
    mock_redis_instance.get.return_value = None  # Cache miss
    mock_redis_instance.setex = AsyncMock()
    mock_redis.return_value = mock_redis_instance
      ]]></code>
    </mockPattern>
  </interfaces>

  <tests>
    <standards>
      <standard>All unit tests run without network/database access</standard>
      <standard>Each test function is independent and isolated</standard>
      <standard>Use @pytest.mark.unit marker on all unit tests</standard>
      <standard>Use @pytest.mark.asyncio for async tests (auto mode configured)</standard>
      <standard>Maintain >80% coverage on service modules</standard>
    </standards>
    <locations>
      <location path="backend/tests/unit/test_draft_service.py">12 tests - status complete</location>
      <location path="backend/tests/unit/test_search_service.py">~25 tests - verify mocks work</location>
      <location path="backend/tests/unit/test_generation_service.py">7 tests - verify streaming mocks</location>
      <location path="backend/tests/unit/test_explanation_service.py">8 tests - verify LLM mocks</location>
      <location path="backend/tests/unit/test_audit_service_queries.py">Fix database session mock</location>
      <location path="backend/tests/unit/test_queue_monitor_service.py">Fix Redis client mock</location>
      <location path="backend/tests/unit/test_kb_recommendation_service.py">Fix database query mock</location>
    </locations>
    <ideas>
      <idea ac="7.6.1">
        Verify DraftService tests use AsyncMock(spec=AsyncSession) for session
      </idea>
      <idea ac="7.6.2">
        Verify search tests mock both embedding_client and qdrant_service
      </idea>
      <idea ac="7.6.3">
        Verify generation tests use streaming mock with __aiter__ pattern
      </idea>
      <idea ac="7.6.4">
        Verify explanation tests handle acompletion timeout with fallback
      </idea>
      <idea ac="7.6.5">
        Document all mock patterns in testing-guideline.md DI section
      </idea>
    </ideas>
  </tests>
</story-context>
